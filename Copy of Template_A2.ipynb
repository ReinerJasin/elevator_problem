{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCNr2rW27nBk"
   },
   "source": [
    "# Team information\n",
    "\n",
    "| Team member 1     | Details  | Team member 2     | Details  |\n",
    "| :---------------- | :------: | :---------------- | :------: |\n",
    "| Name              | Nadia Victoria Aritonang         | Name              | Reiner Anggriawan Jasin |\n",
    "| NUSNet (Exxxxxxx) | E1505949         | NUSNet (Exxxxxxx) | E1503344 |\n",
    "| Matric (AxxxxxxxZ)| A0314698N         | Matric (AxxxxxxxZ)| A0314502W |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1860,
     "status": "ok",
     "timestamp": 1743518511488,
     "user": {
      "displayName": "Reiner Jasin",
      "userId": "00815975572482413347"
     },
     "user_tz": -480
    },
    "id": "NQp3ObqD9d4u",
    "outputId": "f20549aa-847d-413f-a60c-aa741c1b89ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Connect to Google drive to save your model, etc.,\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCmfJeb_BMHM"
   },
   "source": [
    "# Installation and setup\n",
    "\n",
    "The gym environment requires an older version numpy (and corresponding packages). <br>\n",
    "The following cell contains the `requirements.txt` to setup the python environment used in the rest of this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1743518480845,
     "user": {
      "displayName": "Reiner Jasin",
      "userId": "00815975572482413347"
     },
     "user_tz": -480
    },
    "id": "eWitd3VTBgwU",
    "outputId": "9b150fc8-8ec0-4008-f89e-bd8812652986"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "cloudpickle==3.1.1\n",
    "contourpy==1.3.0\n",
    "cycler==0.12.1\n",
    "filelock==3.18.0\n",
    "fonttools==4.56.0\n",
    "fsspec==2025.3.0\n",
    "gym==0.26.2\n",
    "gym-notices==0.0.8\n",
    "importlib_metadata==8.6.1\n",
    "importlib_resources==6.5.2\n",
    "Jinja2==3.1.6\n",
    "kiwisolver==1.4.7\n",
    "MarkupSafe==3.0.2\n",
    "matplotlib==3.9.4\n",
    "mpmath==1.3.0\n",
    "networkx==3.2.1\n",
    "numpy==1.24.2\n",
    "nvidia-cublas-cu12==12.4.5.8\n",
    "nvidia-cuda-cupti-cu12==12.4.127\n",
    "nvidia-cuda-nvrtc-cu12==12.4.127\n",
    "nvidia-cuda-runtime-cu12==12.4.127\n",
    "nvidia-cudnn-cu12==9.1.0.70\n",
    "nvidia-cufft-cu12==11.2.1.3\n",
    "nvidia-curand-cu12==10.3.5.147\n",
    "nvidia-cusolver-cu12==11.6.1.9\n",
    "nvidia-cusparse-cu12==12.3.1.170\n",
    "nvidia-cusparselt-cu12==0.6.2\n",
    "nvidia-nccl-cu12==2.21.5\n",
    "nvidia-nvjitlink-cu12==12.4.127\n",
    "nvidia-nvtx-cu12==12.4.127\n",
    "packaging==24.2\n",
    "pillow==11.1.0\n",
    "ply==3.11\n",
    "pygame==2.6.1\n",
    "pyparsing==3.2.1\n",
    "python-dateutil==2.9.0.post0\n",
    "six==1.17.0\n",
    "sympy==1.13.1\n",
    "torch==2.6.0\n",
    "tqdm==4.67.1\n",
    "triton==3.2.0\n",
    "zipp==3.21.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_9eK2YJBoGb"
   },
   "source": [
    "Now install the requirements.\n",
    "\n",
    "You may be asked to restart the session to load the installed versions of the packages. If so, restart the session and continue using the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 18767,
     "status": "ok",
     "timestamp": 1743518499613,
     "user": {
      "displayName": "Reiner Jasin",
      "userId": "00815975572482413347"
     },
     "user_tz": -480
    },
    "id": "fXtGcN8u94_N",
    "outputId": "eb8654ee-98c7-48b8-855a-6ddf8f352877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudpickle==3.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (3.1.1)\n",
      "Collecting contourpy==1.3.0 (from -r requirements.txt (line 3))\n",
      "  Using cached contourpy-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: filelock==3.18.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.18.0)\n",
      "Requirement already satisfied: fonttools==4.56.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (4.56.0)\n",
      "Requirement already satisfied: fsspec==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (2025.3.0)\n",
      "Requirement already satisfied: gym==0.26.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (0.26.2)\n",
      "Requirement already satisfied: gym-notices==0.0.8 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (0.0.8)\n",
      "Requirement already satisfied: importlib_metadata==8.6.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (8.6.1)\n",
      "Requirement already satisfied: importlib_resources==6.5.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (6.5.2)\n",
      "Requirement already satisfied: Jinja2==3.1.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (3.1.6)\n",
      "Collecting kiwisolver==1.4.7 (from -r requirements.txt (line 13))\n",
      "  Using cached kiwisolver-1.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: MarkupSafe==3.0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (3.0.2)\n",
      "Collecting matplotlib==3.9.4 (from -r requirements.txt (line 15))\n",
      "  Using cached matplotlib-3.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (1.3.0)\n",
      "Requirement already satisfied: networkx==3.2.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (3.2.1)\n",
      "Collecting numpy==1.24.2 (from -r requirements.txt (line 18))\n",
      "  Using cached numpy-1.24.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 29)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 30)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 31)) (12.4.127)\n",
      "Requirement already satisfied: packaging==24.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 32)) (24.2)\n",
      "Collecting pillow==11.1.0 (from -r requirements.txt (line 33))\n",
      "  Downloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: ply==3.11 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 34)) (3.11)\n",
      "Requirement already satisfied: pygame==2.6.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 35)) (2.6.1)\n",
      "Collecting pyparsing==3.2.1 (from -r requirements.txt (line 36))\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 37)) (2.9.0.post0)\n",
      "Requirement already satisfied: six==1.17.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 38)) (1.17.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 39)) (1.13.1)\n",
      "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 40)) (2.6.0+cu124)\n",
      "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 41)) (4.67.1)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (3.2.0)\n",
      "Requirement already satisfied: zipp==3.21.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 43)) (3.21.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 40)) (4.13.0)\n",
      "Using cached contourpy-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Using cached kiwisolver-1.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "Using cached matplotlib-3.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "Using cached numpy-1.24.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Downloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, pillow, numpy, kiwisolver, contourpy, matplotlib\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.2.3\n",
      "    Uninstalling pyparsing-3.2.3:\n",
      "      Successfully uninstalled pyparsing-3.2.3\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 11.2.0\n",
      "    Uninstalling pillow-11.2.0:\n",
      "      Successfully uninstalled pillow-11.2.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.4\n",
      "    Uninstalling numpy-2.2.4:\n",
      "      Successfully uninstalled numpy-2.2.4\n",
      "  Attempting uninstall: kiwisolver\n",
      "    Found existing installation: kiwisolver 1.4.8\n",
      "    Uninstalling kiwisolver-1.4.8:\n",
      "      Successfully uninstalled kiwisolver-1.4.8\n",
      "  Attempting uninstall: contourpy\n",
      "    Found existing installation: contourpy 1.3.1\n",
      "    Uninstalling contourpy-1.3.1:\n",
      "      Successfully uninstalled contourpy-1.3.1\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.10.1\n",
      "    Uninstalling matplotlib-3.10.1:\n",
      "      Successfully uninstalled matplotlib-3.10.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.2 which is incompatible.\n",
      "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.2 which is incompatible.\n",
      "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.2 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\n",
      "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.2 which is incompatible.\n",
      "pymc 5.21.1 requires numpy>=1.25.0, but you have numpy 1.24.2 which is incompatible.\n",
      "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.2 which is incompatible.\n",
      "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.24.2 which is incompatible.\n",
      "blosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.24.2 which is incompatible.\n",
      "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed contourpy-1.3.0 kiwisolver-1.4.7 matplotlib-3.9.4 numpy-1.24.2 pillow-11.1.0 pyparsing-3.2.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "acaae15b7790426f8c7d95aea188ce9f",
       "pip_warning": {
        "packages": [
         "PIL",
         "kiwisolver",
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c13c63a6"
   },
   "source": [
    "We will use a discretized version of\n",
    "the [elevator domain](https://ataitler.github.io/IPPC2023/elevator.html) from the International Planning Competition, 2023.\n",
    "\n",
    "Install the pyRDDL gym environment using the given repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23148,
     "status": "ok",
     "timestamp": 1743518534637,
     "user": {
      "displayName": "Reiner Jasin",
      "userId": "00815975572482413347"
     },
     "user_tz": -480
    },
    "id": "8U02_AG3900U",
    "outputId": "3d069c0b-768a-4948-eb39-c56f7c6f22b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pyRDDLGym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/tasbolat1/pyRDDLGym.git --force-reinstall\n",
    "\n",
    "## Install other packages if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1743523674929,
     "user": {
      "displayName": "Reiner Jasin",
      "userId": "00815975572482413347"
     },
     "user_tz": -480
    },
    "id": "-gknJ0Ud97HT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31435/2533168676.py:14: UserWarning: cv2 is not installed: save_as_mp4 option will be disabled.\n",
      "  from pyRDDLGym.Visualizer.MovieGenerator import MovieGenerator # loads visualizer utilites\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "from pyRDDLGym.Visualizer.MovieGenerator import MovieGenerator # loads visualizer utilites\n",
    "from IPython.display import Image, display, clear_output # for displaying gifs in colab\n",
    "from pyRDDLGym.Elevator import Elevator # imports Discrete Elevator\n",
    "\n",
    "## Add more imports here as required\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGHWCiCnfCO4"
   },
   "source": [
    "# Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4540,
     "status": "ok",
     "timestamp": 1743523679848,
     "user": {
      "displayName": "Reiner Jasin",
      "userId": "00815975572482413347"
     },
     "user_tz": -480
    },
    "id": "o1E0mIDq-LXu",
    "outputId": "cd824916-a2a6-437c-b9a7-243df559cca8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reiner/github/elevator_problem/venv/lib/python3.12/site-packages/pyRDDLGym/Core/Env/RDDLConstraints.py:85: UserWarning: Constraint does not have a structure of <action or state fluent> <op> <rhs>, where:\n",
      "<op> is one of {<=, <, >=, >}\n",
      "<rhs> is a deterministic function of non-fluents or constants only.\n",
      ">> ( sum_{?f: floor} [ elevator-at-floor(?e, ?f) ] ) == 1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/reiner/github/elevator_problem/venv/lib/python3.12/site-packages/pyRDDLGym/Examples /home/reiner/github/elevator_problem/venv/lib/python3.12/site-packages/pyRDDLGym/Examples/manifest.csv\n",
      "Available example environment(s):\n",
      "PropDBN -> Simple propositional DBN.\n",
      "MarsRover -> Multi Rover Navigation, where a group of agent needs to harvest mineral.\n",
      "NewLanguage -> Example with new language features.\n",
      "NewtonZero -> Example with Newton root-finding method.\n",
      "HVAC -> Multi-zone and multi-heater HVAC control problem\n",
      "SupplyChain -> A supply chain with factory and multiple warehouses.\n",
      "SupplyChainNet -> A supply chain network with factory and multiple warehouses.\n",
      "RaceCar -> A simple continuous MDP for the racecar problem.\n",
      "Traffic -> BLX/QTM traffic model.\n",
      "Wildfire -> A boolean version of the wildfire fighting domain.\n",
      "Reservoir_continuous -> Continuous action version of management of the water level in interconnected reservoirs.\n",
      "Reservoir_discrete -> Discrete version of management of the water level in interconnected reservoirs.\n",
      "RecSim -> A problem of recommendation systems, with consumers and providers.\n",
      "Elevators -> The Elevator domain models evening rush hours when people from different floors in a building want to go down to the bottom floor using elevators.\n",
      "PowerGen_continuous -> A continuous simple power generation problem loosely modeled on the problem of unit commitment.\n",
      "PowerGen_discrete -> A simple power generation problem loosely modeled on the problem of unit commitment.\n",
      "UAV_mixed -> Mixed action space version of multi-UAV problem where a group of UAVs have to reach goal positions in the 3d Space.\n",
      "UAV_continuous -> Continuous action space version of multi-UAV problem where a group of UAVs have to reach goal positions in the 3d Space.\n",
      "UAV_discrete -> Discrete action space version of multi-UAV problem where a group of UAVs have to reach goal positions in the 3d Space.\n",
      "CartPole_continuous -> A simple continuous state-action MDP for the classical cart-pole system by Rich Sutton, with actions that describe the continuous force applied to the cart.\n",
      "CartPole_discrete -> A simple continuous state MDP for the classical cart-pole system by Rich Sutton, with discrete actions that apply a constant force on either the left or right side of the cart.\n",
      "MountainCar -> A simple continuous MDP for the classical mountain car control problem.\n",
      "The building has 5 floors and 1 elevators. Each floor has maximum 3 people waiting. Each elevator can carry maximum of 10 people.\n",
      "Discrete environment actions:\n",
      "{0: ('e0_movcurdir_0',), 1: ('e0_movcurdir_1',), 2: ('e0_close_0',), 3: ('e0_close_1',), 4: ('e0_open_0',), 5: ('e0_open_1',)}\n",
      "Continuous environment actions:\n",
      "Dict('move-current-dir___e0': Discrete(2), 'open-door___e0': Discrete(2), 'close-door___e0': Discrete(2))\n",
      "Observation space size for the discrete Elevator Environment: 225280\n"
     ]
    }
   ],
   "source": [
    "## IMPORTANT: Do not change the instance of the environment.\n",
    "env = Elevator(instance = 5)\n",
    "\n",
    "print('Discrete environment actions:')\n",
    "print(env.disc_actions)\n",
    "print('Continuous environment actions:')\n",
    "print(env.base_env.action_space)\n",
    "print(f\"Observation space size for the discrete Elevator Environment: {len(env.disc_states)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e13dca8b"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1743524197664,
     "user": {
      "displayName": "Reiner Jasin",
      "userId": "00815975572482413347"
     },
     "user_tz": -480
    },
    "id": "uZrE28ZRGBmk"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "## IMPORTANT: <BEGIN> DO NOT CHANGE THIS CODE!\n",
    "## GENERAL HYPERPARAMS\n",
    "num_episodes = 3000\n",
    "## IMPORTANT: <END> DO NOT CHANGE THIS CODE!\n",
    "\n",
    "# learning_rate = 3e-4\n",
    "learning_rate = 0.005\n",
    "batch_size = 64\n",
    "clip_value = 1.0  # Gradient clipping value\n",
    "\n",
    "## ALGO SPECIFIC HYPERPARAMS\n",
    "# Update the hyperparams as necessary for your implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53155607"
   },
   "source": [
    "# Model Definition\n",
    "\n",
    "Define your model here. You can rename the class `YourModel` appropriately and use it later in the code.\n",
    "Note: In case of actor-critic or other models, all components must subclass `nn.Module`\n",
    "\n",
    "- Your model should take in 11 inputs, which will be derived from the convert_state_to_list function.\n",
    "- Your model should return 6 values corresponding to action logits or probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1743524198045,
     "user": {
      "displayName": "Reiner Jasin",
      "userId": "00815975572482413347"
     },
     "user_tz": -480
    },
    "id": "2bLSWBgLGEVC"
   },
   "outputs": [],
   "source": [
    "class YourModel(nn.Module):\n",
    "    def __init__(self, input_size=13, hidden_size=128, output_size=6):\n",
    "        super(YourModel, self).__init__()\n",
    "        # Your model layers and initializations here\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x will be a tensor with shape [batch_size, 11]\n",
    "        # Your forward pass logic here\n",
    "        # Ensure the output has shape [batch_size, 6]\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        output = self.fc2(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d501d6e1"
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1743524198475,
     "user": {
      "displayName": "Reiner Jasin",
      "userId": "00815975572482413347"
     },
     "user_tz": -480
    },
    "id": "c96b0591"
   },
   "outputs": [],
   "source": [
    "## IMPORTANT: DO NOT CHANGE THIS CODE!\n",
    "env_features = list(env.observation_space.keys())\n",
    "\n",
    "def convert_state_to_list(state, env_features):\n",
    "    out = []\n",
    "    for i in env_features:\n",
    "        out.append(state[i])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4a67d06"
   },
   "source": [
    "# Neural Net Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1743524199029,
     "user": {
      "displayName": "Reiner Jasin",
      "userId": "00815975572482413347"
     },
     "user_tz": -480
    },
    "id": "9uRwCjl7GHDJ"
   },
   "outputs": [],
   "source": [
    "# Initialize the network and optimizer\n",
    "input_size = len(env_features)\n",
    "output_size = 6\n",
    "\n",
    "# INITIALIZE OTHER NETWORK PARAMS HERE\n",
    "hidden_size = 128\n",
    "\n",
    "# INITIALIZE YOUR NETWORK HERE\n",
    "your_network = YourModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# INIT OPTIMIZER - Adam is a good start, but you can try changing this as well\n",
    "# optimizer = optim.Adam(\n",
    "#     your_network.parameters(), lr=learning_rate\n",
    "# )\n",
    "\n",
    "optimizer = optim.RMSprop(\n",
    "    your_network.parameters(), lr=learning_rate,\n",
    ")\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     your_network.parameters(), lr=learning_rate\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1743524199307,
     "user": {
      "displayName": "Reiner Jasin",
      "userId": "00815975572482413347"
     },
     "user_tz": -480
    },
    "id": "sDKOYvH58igB"
   },
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=num_episodes/100, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1743524199599,
     "user": {
      "displayName": "Reiner Jasin",
      "userId": "00815975572482413347"
     },
     "user_tz": -480
    },
    "id": "xpEQ5uTqGJIQ",
    "outputId": "cccf3e66-ae48-4bd6-f71e-82e2c5de63b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YourModel(\n",
       "  (fc1): Linear(in_features=13, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert networks to CUDA if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "your_network.to(device)\n",
    "\n",
    "# Define other constructs (replay buffers, etc) as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GooOycK-MPib"
   },
   "source": [
    "## Gradient Clipping (Optional, you can use torch's version as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1743524200318,
     "user": {
      "displayName": "Reiner Jasin",
      "userId": "00815975572482413347"
     },
     "user_tz": -480
    },
    "id": "MZM5yTnHMN83"
   },
   "outputs": [],
   "source": [
    "# Define a function for gradient clipping\n",
    "def clip_grads(model, clip_value):\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            param.grad.data = torch.clamp(param.grad.data, -clip_value, clip_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c50a3522"
   },
   "source": [
    "# Live Plotting Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1743524201178,
     "user": {
      "displayName": "Reiner Jasin",
      "userId": "00815975572482413347"
     },
     "user_tz": -480
    },
    "id": "kJTkOusq4bbH"
   },
   "outputs": [],
   "source": [
    "# Create a figure for plotting\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plt.ion()\n",
    "\n",
    "# Lists to store rewards and episode numbers\n",
    "rewards_list = []\n",
    "episodes = []\n",
    "\n",
    "def exponential_smoothing(data, alpha=0.1):\n",
    "    \"\"\"Compute exponential smoothing.\"\"\"\n",
    "    smoothed = [data[0]]  # Initialize with the first data point\n",
    "    for i in range(1, len(data)):\n",
    "        st = alpha * data[i] + (1 - alpha) * smoothed[-1]\n",
    "        smoothed.append(st)\n",
    "    return smoothed\n",
    "\n",
    "def live_plot(data_dict, figure, ylabel=\"Total Rewards\"):\n",
    "    \"\"\"Plot the live graph.\"\"\"\n",
    "    clear_output(wait=True)\n",
    "    ax.clear()\n",
    "    for label, data in data_dict.items():\n",
    "        if label == \"Total Reward\":\n",
    "            ax.plot(data, label=label, color=\"yellow\", linestyle='--')\n",
    "\n",
    "            # Compute and plot moving average for total reward\n",
    "            ma = exponential_smoothing(data)\n",
    "            ma_idx_start = len(data) - len(ma)\n",
    "            ax.plot(range(ma_idx_start, len(data)), ma, label=\"Smoothed Value\", linestyle=\"-\", color=\"purple\", linewidth=2)\n",
    "        else:\n",
    "            ax.plot(data, label=label)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend(loc='upper left')\n",
    "    display(figure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "232d30e3"
   },
   "source": [
    "# RL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1743524202009,
     "user": {
      "displayName": "Reiner Jasin",
      "userId": "00815975572482413347"
     },
     "user_tz": -480
    },
    "id": "fItfNTEx8Luf"
   },
   "outputs": [],
   "source": [
    "# Define the loss calculation function\n",
    "def calculate_loss(batch):\n",
    "    ## TODO - CALCULATE LOSS VALUE & RETURN IT\n",
    "    state_tensor, action, reward, next_state_tensor, done = zip(*batch)\n",
    "\n",
    "    state_tensor = torch.tensor(state_tensor, dtype=torch.float32, device=device)\n",
    "    action = torch.tensor(action, dtype=torch.long, device=device)\n",
    "    reward = torch.tensor(reward, dtype=torch.float32, device=device)\n",
    "    next_state_tensor = torch.tensor(next_state_tensor, dtype=torch.float32, device=device)\n",
    "    done = torch.tensor(done, dtype=torch.float16, device=device)\n",
    "\n",
    "    q_values = your_network(state_tensor)\n",
    "    q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q_values = your_network(next_state_tensor)\n",
    "        next_q_value = next_q_values.max(1)[0]\n",
    "        target_q_value = reward + (1 - done) * 0.99 * next_q_value  # Discounted reward\n",
    "\n",
    "    loss = F.mse_loss(q_value, target_q_value.detach())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1743524202511,
     "user": {
      "displayName": "Reiner Jasin",
      "userId": "00815975572482413347"
     },
     "user_tz": -480
    },
    "id": "mV629tHLL6GQ"
   },
   "outputs": [],
   "source": [
    "def choose_action(state_tensor, epsilon=0.1):\n",
    "    ## TODO - RETURN AN INTEGER FROM 0 - 5 (both inclusive) based on your model training/testing strategy\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "      return random.randint(0, output_size - 1)\n",
    "    else:\n",
    "      state_dimension = state_tensor.unsqueeze(0)\n",
    "\n",
    "      with torch.no_grad():\n",
    "        q_values = your_network(state_dimension)\n",
    "\n",
    "      return torch.argmax(q_values).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1743524203135,
     "user": {
      "displayName": "Reiner Jasin",
      "userId": "00815975572482413347"
     },
     "user_tz": -480
    },
    "id": "E6ryIIG31okB"
   },
   "outputs": [],
   "source": [
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "memory = ReplayMemory(10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKakuRs4ELDu"
   },
   "source": [
    "## Training loop with live plotting\n",
    "\n",
    "Use the graph generated here in your pdf submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "aVy8Zc8vGV7D",
    "outputId": "300a9ac0-2df3-43c6-ef90-123e6bacc0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s, Loss=0, Total Reward=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s, Loss=0, Total Reward=0]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FigureCanvasAgg' object has no attribute 'tostring_rgb'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[32m     10\u001b[39m     total_reward = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     state = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     14\u001b[39m         \u001b[38;5;66;03m# Convert the original state to the suitable format for the network\u001b[39;00m\n\u001b[32m     15\u001b[39m         state_desc = env.disc2state(state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/elevator_problem/venv/lib/python3.12/site-packages/pyRDDLGym/Elevator.py:75\u001b[39m, in \u001b[36mElevator.reset\u001b[39m\u001b[34m(self, seed)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, seed=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state2disc(state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/elevator_problem/venv/lib/python3.12/site-packages/pyRDDLGym/Core/Env/RDDLEnv.py:310\u001b[39m, in \u001b[36mRDDLEnv.reset\u001b[39m\u001b[34m(self, seed, exp_state, vis)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28mself\u001b[39m.state = \u001b[38;5;28mself\u001b[39m.sampler.states\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m vis:\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_visualizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._movie_generator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._movie_per_episode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/elevator_problem/venv/lib/python3.12/site-packages/pyRDDLGym/Visualizer/TextViz.py:86\u001b[39m, in \u001b[36mTextVisualizer.render\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     78\u001b[39m text_str = pprint.pformat(text_layout)[\u001b[32m1\u001b[39m:-\u001b[32m1\u001b[39m]\n\u001b[32m     79\u001b[39m \u001b[38;5;28mself\u001b[39m._ax.text(\u001b[38;5;28mself\u001b[39m._interval * \u001b[32m0.5\u001b[39m,\n\u001b[32m     80\u001b[39m               \u001b[38;5;28mself\u001b[39m._figure_size[\u001b[32m1\u001b[39m] * \u001b[38;5;28mself\u001b[39m._interval * \u001b[32m0.95\u001b[39m,\n\u001b[32m     81\u001b[39m               text_str,\n\u001b[32m     82\u001b[39m               horizontalalignment=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     83\u001b[39m               verticalalignment=\u001b[33m'\u001b[39m\u001b[33mtop\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     84\u001b[39m               wrap=\u001b[38;5;28;01mTrue\u001b[39;00m, fontsize=\u001b[38;5;28mself\u001b[39m._fontsize)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert2img\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ax\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m._ax.cla()\n\u001b[32m     89\u001b[39m plt.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/elevator_problem/venv/lib/python3.12/site-packages/pyRDDLGym/Visualizer/TextViz.py:60\u001b[39m, in \u001b[36mTextVisualizer.convert2img\u001b[39m\u001b[34m(self, fig, ax)\u001b[39m\n\u001b[32m     57\u001b[39m ax.set_position((\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m     58\u001b[39m fig.canvas.draw()\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m data = np.frombuffer(\u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcanvas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtostring_rgb\u001b[49m(), dtype=np.uint8)\n\u001b[32m     61\u001b[39m data = data.reshape(fig.canvas.get_width_height()[::-\u001b[32m1\u001b[39m] + (\u001b[32m3\u001b[39m,))\n\u001b[32m     63\u001b[39m img = Image.fromarray(data)\n",
      "\u001b[31mAttributeError\u001b[39m: 'FigureCanvasAgg' object has no attribute 'tostring_rgb'"
     ]
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plt.ion()\n",
    "\n",
    "# Create a tqdm progress bar\n",
    "progress_bar = tqdm.tqdm(range(num_episodes), postfix={'Total Reward': 0, 'Loss': 0})\n",
    "\n",
    "# RL algorithm training loop\n",
    "for episode in progress_bar:\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # Convert the original state to the suitable format for the network\n",
    "        state_desc = env.disc2state(state)\n",
    "        state_list = convert_state_to_list(state_desc, env_features)\n",
    "        state_tensor = torch.tensor(state_list, dtype=torch.float32, device=device)\n",
    "\n",
    "        action = choose_action(state_tensor, epsilon=max(0.01, 0.1 * (0.99 ** episode)))\n",
    "\n",
    "        # Take the chosen action and observe the next state and reward\n",
    "        next_state, reward, done, _ = env.step((action))\n",
    "\n",
    "        # Convert the next state to the suitable format for the network\n",
    "        next_state_desc = env.disc2state(next_state)\n",
    "        next_state_list = convert_state_to_list(next_state_desc, env_features)\n",
    "        next_state_tensor = torch.tensor(next_state_list, dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "        # Hint: You may want to collect experiences from the environment to update the agent in batches!\n",
    "\n",
    "        memory.push(state_list, action, reward, next_state_list, done)\n",
    "\n",
    "        if len(memory) > batch_size:\n",
    "            batch = memory.sample(batch_size)\n",
    "            # print(f'batch {batch}')\n",
    "            loss = calculate_loss(batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            clip_grads(your_network, clip_value)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "    rewards_list.append(total_reward)\n",
    "    episodes.append(episode)\n",
    "\n",
    "    live_plot({'Total Reward': rewards_list}, fig)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Saving the model\n",
    "    if episode%500 == 0:\n",
    "      torch.save(your_network, f'model.pt')\n",
    "\n",
    "    progress_bar.set_postfix({'Total Reward': total_reward, 'Loss': loss.item(), 'lr': scheduler.get_lr()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp0VFEQpF57M"
   },
   "source": [
    "## Compute the mean rewards\n",
    "\n",
    "Report the mean rewards obtained in your pdf submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCL1WgMHF86n"
   },
   "outputs": [],
   "source": [
    "print(f\"\\nMean Rewards: ...\")\n",
    "\n",
    "# close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-69AA00ddPBO"
   },
   "source": [
    "# HMM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 335040,
     "status": "aborted",
     "timestamp": 1743523443055,
     "user": {
      "displayName": "Reiner Jasin",
      "userId": "00815975572482413347"
     },
     "user_tz": -480
    },
    "id": "EYgPBNCidOzs"
   },
   "outputs": [],
   "source": [
    "# Connect to Google drive to save your model, etc.,\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "from pyRDDLGym.Visualizer.MovieGenerator import MovieGenerator # loads visualizer utilites\n",
    "from IPython.display import Image, display, clear_output # for displaying gifs in colab\n",
    "from pyRDDLGym.Elevator import Elevator # imports Discrete Elevator\n",
    "\n",
    "## Add more imports here as required\n",
    "\n",
    "## IMPORTANT: Do not change the instance of the environment.\n",
    "env = Elevator(instance = 5)\n",
    "\n",
    "print('Discrete environment actions:')\n",
    "print(env.disc_actions)\n",
    "print('Continuous environment actions:')\n",
    "print(env.base_env.action_space)\n",
    "print(f\"Observation space size for the discrete Elevator Environment: {len(env.disc_states)}\")\n",
    "\n",
    "# Define hyperparameters\n",
    "\n",
    "## IMPORTANT: <BEGIN> DO NOT CHANGE THIS CODE!\n",
    "## GENERAL HYPERPARAMS\n",
    "num_episodes = 3000\n",
    "## IMPORTANT: <END> DO NOT CHANGE THIS CODE!\n",
    "\n",
    "learning_rate = 3e-4\n",
    "batch_size = 64\n",
    "clip_value = 1.0  # Gradient clipping value\n",
    "\n",
    "## ALGO SPECIFIC HYPERPARAMS\n",
    "# Update the hyperparams as necessary for your implementation\n",
    "\n",
    "class YourModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(YourModel, self).__init__()\n",
    "        # Your model layers and initializations here\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x will be a tensor with shape [batch_size, 13]\n",
    "        # Your forward pass logic here\n",
    "        # Ensure the output has shape [batch_size, 6]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        output = self.fc2(x)\n",
    "        return output\n",
    "\n",
    "## IMPORTANT: DO NOT CHANGE THIS CODE!\n",
    "env_features = list(env.observation_space.keys())\n",
    "\n",
    "def convert_state_to_list(state, env_features):\n",
    "    out = []\n",
    "    for i in env_features:\n",
    "        out.append(state[i])\n",
    "    return out\n",
    "\n",
    "for feature in env_features:\n",
    "  print(f\"{feature}: {env.observation_space[feature]}\")\n",
    "\n",
    "# Initialize the network and optimizer\n",
    "input_size = len(env_features)\n",
    "output_size = 6\n",
    "\n",
    "# INITIALIZE OTHER NETWORK PARAMS HERE\n",
    "hidden_size = 128\n",
    "\n",
    "# INITIALIZE YOUR NETWORK HERE\n",
    "your_network = YourModel()\n",
    "\n",
    "# INIT OPTIMIZER - Adam is a good start, but you can try changing this as well\n",
    "optimizer = optim.Adam(\n",
    "    your_network.parameters(), lr=learning_rate\n",
    ")\n",
    "\n",
    "# Convert networks to CUDA if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "your_network.to(device)\n",
    "\n",
    "# Define other constructs (replay buffers, etc) as necessary\n",
    "\n",
    "# Define a function for gradient clipping\n",
    "def clip_grads(model, clip_value):\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            param.grad.data = torch.clamp(param.grad.data, -clip_value, clip_value)\n",
    "\n",
    "# Create a figure for plotting\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plt.ion()\n",
    "\n",
    "# Lists to store rewards and episode numbers\n",
    "rewards_list = []\n",
    "episodes = []\n",
    "\n",
    "def exponential_smoothing(data, alpha=0.1):\n",
    "    \"\"\"Compute exponential smoothing.\"\"\"\n",
    "    smoothed = [data[0]]  # Initialize with the first data point\n",
    "    for i in range(1, len(data)):\n",
    "        st = alpha * data[i] + (1 - alpha) * smoothed[-1]\n",
    "        smoothed.append(st)\n",
    "    return smoothed\n",
    "\n",
    "def live_plot(data_dict, figure, ylabel=\"Total Rewards\"):\n",
    "    \"\"\"Plot the live graph.\"\"\"\n",
    "    clear_output(wait=True)\n",
    "    ax.clear()\n",
    "    for label, data in data_dict.items():\n",
    "        if label == \"Total Reward\":\n",
    "            ax.plot(data, label=label, color=\"yellow\", linestyle='--')\n",
    "\n",
    "            # Compute and plot moving average for total reward\n",
    "            ma = exponential_smoothing(data)\n",
    "            ma_idx_start = len(data) - len(ma)\n",
    "            ax.plot(range(ma_idx_start, len(data)), ma, label=\"Smoothed Value\", linestyle=\"-\", color=\"purple\", linewidth=2)\n",
    "        else:\n",
    "            ax.plot(data, label=label)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend(loc='upper left')\n",
    "    display(figure)\n",
    "\n",
    "# Define the loss calculation function\n",
    "def calculate_loss(\n",
    "    ## INCLUDE PARAMS YOU NEED HERE\n",
    "    state_tensor, action, reward, next_state_tensor, done\n",
    "    ):\n",
    "    ## TODO - CALCULATE LOSS VALUE & RETURN IT\n",
    "    q_values = your_network(state_tensor)\n",
    "    q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      next_q_values = your_network(next_state_tensor)\n",
    "      next_q_value = next_q_values.max(1)[0]\n",
    "      target_q_value = reward + (1 - done) * 0.99 * next_q_value\n",
    "\n",
    "    loss = F.mse_loss(q_value, target_q_value)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def choose_action(\n",
    "    ## INCLUDE PARAMS YOU NEED HERE\n",
    "    state_tensor, epsilon=0.1\n",
    "    ):\n",
    "    ## TODO - RETURN AN INTEGER FROM 0 - 5 (both inclusive) based on your model training/testing strategy\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "      return torch.tensor(random.choice(range(output_size)), dtype=torch.long, device=device)\n",
    "    else:\n",
    "      q_values = your_network(state_tensor)\n",
    "      return q_values.argmax().unsqueeze(0)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plt.ion()\n",
    "\n",
    "# Create a tqdm progress bar\n",
    "progress_bar = tqdm.tqdm(range(num_episodes), postfix={'Total Reward': 0, 'Loss': 0})\n",
    "\n",
    "# RL algorithm training loop\n",
    "for episode in progress_bar:\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # Convert the original state to the suitable format for the network\n",
    "        state_desc = env.disc2state(state)\n",
    "        state_list = convert_state_to_list(state_desc, env_features)\n",
    "        state_tensor = torch.tensor(state_list, dtype=torch.float32, device=device)\n",
    "\n",
    "        action = choose_action(\n",
    "            ## TODO: FILL IN PARAMS FOR CALLING choose_action\n",
    "            state_tensor, epsilon=0.1\n",
    "        )\n",
    "\n",
    "        # Take the chosen action and observe the next state and reward\n",
    "        next_state, reward, done, _ = env.step((action))\n",
    "\n",
    "        # Convert the next state to the suitable format for the network\n",
    "        next_state_desc = env.disc2state(next_state)\n",
    "        next_state_list = convert_state_to_list(next_state_desc, env_features)\n",
    "        next_state_tensor = torch.tensor(next_state_list, dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "        # Hint: You may want to collect experiences from the environment to update the agent in batches!\n",
    "\n",
    "        loss = calculate_loss(\n",
    "            ## TODO: FILL IN PARAMS FOR CALLING calculate_loss\n",
    "            state_tensor, action, reward, next_state_tensor, done\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        clip_grads(your_network, clip_value)\n",
    "        optimizer.step()\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "    rewards_list.append(total_reward)\n",
    "    episodes.append(episode)\n",
    "\n",
    "    live_plot({'Total Reward': rewards_list}, fig)\n",
    "\n",
    "    # Saving the model\n",
    "    if episode%500 == 0:\n",
    "      torch.save(your_network, f'model.pt')\n",
    "\n",
    "    progress_bar.set_postfix({'Total Reward': total_reward, 'Loss': loss.item()})\n",
    "\n",
    "print(f\"\\nMean Rewards: {np.mean(rewards_list)}\")\n",
    "\n",
    "# close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyOf4bIfduhp"
   },
   "outputs": [],
   "source": [
    "# Connect to Google drive to save your model, etc.,\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "from pyRDDLGym.Visualizer.MovieGenerator import MovieGenerator # loads visualizer utilites\n",
    "from IPython.display import Image, display, clear_output # for displaying gifs in colab\n",
    "from pyRDDLGym.Elevator import Elevator # imports Discrete Elevator\n",
    "\n",
    "## Add more imports here as required\n",
    "\n",
    "## IMPORTANT: Do not change the instance of the environment.\n",
    "env = Elevator(instance = 5)\n",
    "\n",
    "print('Discrete environment actions:')\n",
    "print(env.disc_actions)\n",
    "print('Continuous environment actions:')\n",
    "print(env.base_env.action_space)\n",
    "print(f\"Observation space size for the discrete Elevator Environment: {len(env.disc_states)}\")\n",
    "\n",
    "# Define hyperparameters\n",
    "\n",
    "## IMPORTANT: <BEGIN> DO NOT CHANGE THIS CODE!\n",
    "## GENERAL HYPERPARAMS\n",
    "num_episodes = 3000\n",
    "## IMPORTANT: <END> DO NOT CHANGE THIS CODE!\n",
    "\n",
    "learning_rate = 3e-4\n",
    "batch_size = 64\n",
    "clip_value = 1.0  # Gradient clipping value\n",
    "\n",
    "## ALGO SPECIFIC HYPERPARAMS\n",
    "# Update the hyperparams as necessary for your implementation\n",
    "\n",
    "class YourModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(YourModel, self).__init__()\n",
    "        # Your model layers and initializations here\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x will be a tensor with shape [batch_size, 13]\n",
    "        # Your forward pass logic here\n",
    "        # Ensure the output has shape [batch_size, 6]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        output = self.fc2(x)\n",
    "        return output\n",
    "\n",
    "## IMPORTANT: DO NOT CHANGE THIS CODE!\n",
    "env_features = list(env.observation_space.keys())\n",
    "\n",
    "def convert_state_to_list(state, env_features):\n",
    "    out = []\n",
    "    for i in env_features:\n",
    "        out.append(state[i])\n",
    "    return out\n",
    "\n",
    "for feature in env_features:\n",
    "  print(f\"{feature}: {env.observation_space[feature]}\")\n",
    "\n",
    "# Initialize the network and optimizer\n",
    "input_size = len(env_features)\n",
    "output_size = 6\n",
    "\n",
    "# INITIALIZE OTHER NETWORK PARAMS HERE\n",
    "hidden_size = 128\n",
    "\n",
    "# INITIALIZE YOUR NETWORK HERE\n",
    "your_network = YourModel()\n",
    "\n",
    "# INIT OPTIMIZER - Adam is a good start, but you can try changing this as well\n",
    "optimizer = optim.Adam(\n",
    "    your_network.parameters(), lr=learning_rate\n",
    ")\n",
    "\n",
    "# Convert networks to CUDA if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "your_network.to(device)\n",
    "\n",
    "# Define other constructs (replay buffers, etc) as necessary\n",
    "\n",
    "# Define a function for gradient clipping\n",
    "def clip_grads(model, clip_value):\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            param.grad.data = torch.clamp(param.grad.data, -clip_value, clip_value)\n",
    "\n",
    "# Create a figure for plotting\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plt.ion()\n",
    "\n",
    "# Lists to store rewards and episode numbers\n",
    "rewards_list = []\n",
    "episodes = []\n",
    "\n",
    "def exponential_smoothing(data, alpha=0.1):\n",
    "    \"\"\"Compute exponential smoothing.\"\"\"\n",
    "    smoothed = [data[0]]  # Initialize with the first data point\n",
    "    for i in range(1, len(data)):\n",
    "        st = alpha * data[i] + (1 - alpha) * smoothed[-1]\n",
    "        smoothed.append(st)\n",
    "    return smoothed\n",
    "\n",
    "def live_plot(data_dict, figure, ylabel=\"Total Rewards\"):\n",
    "    \"\"\"Plot the live graph.\"\"\"\n",
    "    clear_output(wait=True)\n",
    "    ax.clear()\n",
    "    for label, data in data_dict.items():\n",
    "        if label == \"Total Reward\":\n",
    "            ax.plot(data, label=label, color=\"yellow\", linestyle='--')\n",
    "\n",
    "            # Compute and plot moving average for total reward\n",
    "            ma = exponential_smoothing(data)\n",
    "            ma_idx_start = len(data) - len(ma)\n",
    "            ax.plot(range(ma_idx_start, len(data)), ma, label=\"Smoothed Value\", linestyle=\"-\", color=\"purple\", linewidth=2)\n",
    "        else:\n",
    "            ax.plot(data, label=label)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend(loc='upper left')\n",
    "    display(figure)\n",
    "\n",
    "# Define the loss calculation function\n",
    "def calculate_loss(\n",
    "    ## INCLUDE PARAMS YOU NEED HERE\n",
    "    state_tensor, action, reward, next_state_tensor, done\n",
    "    ):\n",
    "    ## TODO - CALCULATE LOSS VALUE & RETURN IT\n",
    "    q_values = your_network(state_tensor)\n",
    "    q_value = q_values.gather(1, action).squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      next_q_values = your_network(next_state_tensor)\n",
    "      next_q_value = next_q_values.max(1)[0]\n",
    "      target_q_value = reward + (1 - done) * 0.99 * next_q_value\n",
    "\n",
    "    loss = F.mse_loss(q_value, target_q_value)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def choose_action(\n",
    "    ## INCLUDE PARAMS YOU NEED HERE\n",
    "    state_tensor, epsilon=0.1\n",
    "    ):\n",
    "    ## TODO - RETURN AN INTEGER FROM 0 - 5 (both inclusive) based on your model training/testing strategy\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "      return torch.tensor(random.choice(range(output_size)), dtype=torch.long, device=device)\n",
    "    else:\n",
    "      q_values = your_network(state_tensor)\n",
    "      return q_values.argmax().unsqueeze(0)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plt.ion()\n",
    "\n",
    "# Create a tqdm progress bar\n",
    "progress_bar = tqdm.tqdm(range(num_episodes), postfix={'Total Reward': 0, 'Loss': 0})\n",
    "\n",
    "# RL algorithm training loop\n",
    "for episode in progress_bar:\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # Convert the original state to the suitable format for the network\n",
    "        state_desc = env.disc2state(state)\n",
    "        state_list = convert_state_to_list(state_desc, env_features)\n",
    "        state_tensor = torch.tensor(state_list, dtype=torch.float32, device=device)\n",
    "\n",
    "        action = choose_action(\n",
    "            ## TODO: FILL IN PARAMS FOR CALLING choose_action\n",
    "            state_tensor, epsilon=0.1\n",
    "        )\n",
    "\n",
    "        # Take the chosen action and observe the next state and reward\n",
    "        next_state, reward, done, _ = env.step((action.item()))\n",
    "\n",
    "        # Convert the next state to the suitable format for the network\n",
    "        next_state_desc = env.disc2state(next_state)\n",
    "        next_state_list = convert_state_to_list(next_state_desc, env_features)\n",
    "        next_state_tensor = torch.tensor(next_state_list, dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "        # Hint: You may want to collect experiences from the environment to update the agent in batches!\n",
    "\n",
    "        loss = calculate_loss(\n",
    "            ## TODO: FILL IN PARAMS FOR CALLING calculate_loss\n",
    "            state_tensor, action, reward, next_state_tensor, done\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        clip_grads(your_network, clip_value)\n",
    "        optimizer.step()\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "    rewards_list.append(total_reward)\n",
    "    episodes.append(episode)\n",
    "\n",
    "    live_plot({'Total Reward': rewards_list}, fig)\n",
    "\n",
    "    # Saving the model\n",
    "    if episode%500 == 0:\n",
    "      torch.save(your_network, f'model.pt')\n",
    "\n",
    "    progress_bar.set_postfix({'Total Reward': total_reward, 'Loss': loss.item()})\n",
    "\n",
    "print(f\"\\nMean Rewards: {np.mean(rewards_list)}\")\n",
    "\n",
    "# close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfI-kD_7jnAc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1dlHXiYO1Dk0Za6Pl5uvgv1LXxMtmsBD8",
     "timestamp": 1743218710369
    },
    {
     "file_id": "1BB95k_jZD36PSg_4C25rzwqzm8f-r5XZ",
     "timestamp": 1742631305010
    },
    {
     "file_id": "1W4WkoRJcbcj91Sl1xPTMSqlvOVkjDTgJ",
     "timestamp": 1722414528738
    },
    {
     "file_id": "1wqEXSH4KTZxnshpUl3Bz4-w_8pxmKMq_",
     "timestamp": 1698138516936
    }
   ]
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
